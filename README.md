# ğŸ¤– IA Locale â€” FastAPI & Ollama

ğŸ§  **API dâ€™intelligence artificielle locale**
ğŸ”’ DonnÃ©es privÃ©es â€¢ âš¡ Rapide â€¢ ğŸ–¥ï¸ 100 % local

Projet IA basÃ© sur **Ollama** et **FastAPI**, permettant dâ€™exÃ©cuter un **LLM local (Llama 3.1)** et des **embeddings sÃ©mantiques**, sans dÃ©pendance au cloud.

---

## ğŸ¯ Objectif du projet

CrÃ©er une API capable de :

âœ”ï¸ Interroger un modÃ¨le de langage local
âœ”ï¸ GÃ©nÃ©rer des rÃ©ponses textuelles
âœ”ï¸ Exploiter des embeddings sÃ©mantiques
âœ”ï¸ Fonctionner sans API externe

ğŸ‘‰ Projet adaptÃ© pour :
- Intranet
- Portfolio
- Projets sensibles
- Recherche & DÃ©veloppement IA

---

## ğŸ› ï¸ Stack technique

ğŸ§  **Ollama** â€” moteur IA local
ğŸ¤– **Llama 3.1 (8B)** â€” modÃ¨le de langage
ğŸ“ **Nomic Embed Text** â€” embeddings sÃ©mantiques
âš™ï¸ **FastAPI** â€” framework API Python
ğŸš€ **Uvicorn** â€” serveur ASGI
ğŸ **Python â‰¥ 3.10**
ğŸ–¥ï¸ **Linux / WSL**

---

## ğŸ“¦ Installation dâ€™Ollama

### Installation

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

VÃ©rification
```bash
ollama --version
```

Lancement du service
```bash
ollama serve
```

âš ï¸ Le service Ollama doit rester actif pendant lâ€™utilisation de lâ€™API.

ğŸ§  Installation des modÃ¨les IA
ModÃ¨le de langage
```bash
ollama pull llama3.1:8b
```

ModÃ¨le dâ€™embeddings
```bash
ollama pull nomic-embed-text
```

VÃ©rification
```bash
ollama list
```


â¡ï¸ Tous les modÃ¨les sont stockÃ©s localement
â¡ï¸ Aucune donnÃ©e nâ€™est envoyÃ©e vers le cloud

ğŸ Configuration de lâ€™environnement Python
CrÃ©ation de lâ€™environnement virtuel
```python
python3 -m venv venv
source venv/bin/activate
```

Mise Ã  jour de pip
```bash
pip install --upgrade pip
```
Installation des dÃ©pendances
```bash
pip install -r requirements.txt
```

Si le fichier requirements.txt nâ€™existe pas encore :

```bash
pip install fastapi uvicorn requests
```

ğŸš€ Lancement de lâ€™API
DÃ©marrage du serveur FastAPI
```bash
uvicorn app:app --reload --port 8001
```
AccÃ¨s Ã  lâ€™application

ğŸŒ API :
http://127.0.0.1:8001

ğŸ“š Documentation Swagger :
http://127.0.0.1:8001/docs

ğŸ§© Architecture du projet
```bash
ia/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ venv/
```

ğŸ“¦ Architecture simple et lisible
ğŸ”§ Facilement extensible (Docker, UI, sÃ©curitÃ©)

ğŸ” Pourquoi une IA locale ?

âœ… ConfidentialitÃ© totale des donnÃ©es
âœ… Aucun coÃ»t dâ€™API externe
âœ… Fonctionnement hors ligne
âœ… ContrÃ´le complet de lâ€™infrastructure
âœ… Performances constantes

ğŸš€ Ã‰volutions possibles

ğŸ’¬ Interface web (Chat UI)
ğŸ” Authentification JWT
ğŸ§  MÃ©moire conversationnelle
ğŸ“Š Recherche sÃ©mantique avancÃ©e
ğŸ³ Docker / Docker Compose
ğŸŒ Reverse proxy Apache + HTTPS

```bash
.curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.1:8b
ollama pull nomic-embed-text

uvicorn app:app --reload --port 8001
```
ouvrir
http://127.0.0.1:8001/docs
